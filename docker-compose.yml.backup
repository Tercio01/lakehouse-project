version: '3.8'

services:
  # MinIO (Substituto do Azure Blob Storage/S3)
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password123
    volumes:
      - ./storage/minio:/data
    command: server /data --console-address ":9001"
    networks:
      - lakehouse-net

  # Spark Cluster (Substituto do Databricks) - PORTAS ALTERADAS
  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    ports:
      - "8180:8080"  # Spark Web UI (alterada de 8080 para 8180)
      - "7077:7077"  # Spark Master
    environment:
      - SPARK_MODE=master
    volumes:
      - ./notebooks:/opt/notebooks
      - ./scripts:/opt/scripts
      - ./data:/opt/data
    networks:
      - lakehouse-net

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=512M
    volumes:
      - ./notebooks:/opt/notebooks
      - ./scripts:/opt/scripts
      - ./data:/opt/data
    networks:
      - lakehouse-net

  # JupyterLab (Para notebooks) - PORTA ALTERADA
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    ports:
      - "8889:8888"  # Alterada de 8888 para 8889
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./scripts:/home/jovyan/scripts
      - ./data:/home/jovyan/data
    environment:
      - JUPYTER_ENABLE_LAB=yes
    networks:
      - lakehouse-net

  # Airflow (Orquestração) - PORTA ALTERADA
  airflow:
    image: apache/airflow:2.5.1
    container_name: airflow
    ports:
      - "8082:8080"  # Alterada de 8081 para 8082
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__RBAC=false
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
    command: >
      bash -c "airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
      airflow webserver & airflow scheduler"
    networks:
      - lakehouse-net

networks:
  lakehouse-net:
    driver: bridge

volumes:
  minio-data:
  spark-data:
